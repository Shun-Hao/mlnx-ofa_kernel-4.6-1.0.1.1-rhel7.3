From: Talat Batheesh <talatb@mellanox.com>
Subject: [PATCH] BACKPORT: drivers/net/ethernet/mellanox/mlx4/en_tx.c

Change-Id: I8d7d4246982133d450eee4c27fc3537df7be9c21
---
 drivers/net/ethernet/mellanox/mlx4/en_tx.c | 104 +++++++++++++++++++++++++++++
 1 file changed, 104 insertions(+)

diff --git a/drivers/net/ethernet/mellanox/mlx4/en_tx.c b/drivers/net/ethernet/mellanox/mlx4/en_tx.c
index xxxxxxx..xxxxxxx 100644
--- a/drivers/net/ethernet/mellanox/mlx4/en_tx.c
+++ b/drivers/net/ethernet/mellanox/mlx4/en_tx.c
@@ -43,6 +43,9 @@
 #include <linux/ip.h>
 #include <linux/ipv6.h>
 #include <linux/moduleparam.h>
+#ifdef HAVE_XDP_ENABLE
+#include <linux/bpf.h>
+#endif
 
 #include "mlx4_en.h"
 
@@ -112,7 +115,11 @@ int mlx4_en_create_tx_ring(struct mlx4_en_priv *priv,
 		goto err_hwq_res;
 	}
 
+#ifdef HAVE_MEMALLOC_NOIO_SAVE
 	err = mlx4_qp_alloc(mdev->dev, ring->qpn, &ring->sp_qp);
+#else
+	err = mlx4_qp_alloc(mdev->dev, ring->qpn, &ring->sp_qp, GFP_KERNEL);
+#endif
 	if (err) {
 		en_err(priv, "Failed allocating qp %d\n", ring->qpn);
 		goto err_reserve;
@@ -327,7 +334,11 @@ u32 mlx4_en_free_tx_desc(struct mlx4_en_priv *priv,
 			}
 		}
 	}
+#ifdef HAVE_NAPI_CONSUME_SKB
 	napi_consume_skb(skb, napi_mode);
+#else
+	dev_kfree_skb(skb);
+#endif
 
 	return tx_info->nr_txbb;
 }
@@ -407,7 +418,13 @@ bool _mlx4_en_process_tx_cq(struct net_device *dev,
 	if (unlikely(!priv->port_up))
 		return true;
 
+#ifdef HAVE_NETDEV_TXQ_BQL_PREFETCHW
 	netdev_txq_bql_complete_prefetchw(ring->tx_queue);
+#else
+#ifdef CONFIG_BQL
+	prefetchw(&ring->tx_queue->dql.limit);
+#endif
+#endif
 
 	index = cons_index & size_mask;
 	cqe = mlx4_en_get_cqe(buf, index, priv->cqe_size) + factor;
@@ -425,7 +442,11 @@ bool _mlx4_en_process_tx_cq(struct net_device *dev,
 		 * make sure we read the CQE after we read the
 		 * ownership bit
 		 */
+#ifdef dma_rmb
 		dma_rmb();
+#else
+		rmb();
+#endif
 
 		if (unlikely((cqe->owner_sr_opcode & MLX4_CQE_OPCODE_MASK) ==
 			     MLX4_CQE_OPCODE_ERROR)) {
@@ -623,9 +644,11 @@ static int get_real_size(const struct sk_buff *skb,
 
 	if (shinfo->gso_size) {
 		*inline_ok = false;
+#ifdef HAVE_SKB_INNER_TRANSPORT_HEADER
 		if (skb->encapsulation)
 			*lso_header_size = (skb_inner_transport_header(skb) - skb->data) + inner_tcp_hdrlen(skb);
 		else
+#endif
 			*lso_header_size = skb_transport_offset(skb) + tcp_hdrlen(skb);
 		real_size = CTRL_SIZE + shinfo->nr_frags * DS_SIZE +
 			ALIGN(*lso_header_size + 4, DS_SIZE);
@@ -699,22 +722,44 @@ static void build_inline_wqe(struct mlx4_en_tx_desc *tx_desc,
 				       skb_frag_size(&shinfo->frags[0]));
 		}
 
+#ifdef dma_wmb
 		dma_wmb();
+#else
+		wmb();
+#endif
 		inl->byte_count = cpu_to_be32(1 << 31 | (skb->len - spc));
 	}
 }
 
+#if defined(NDO_SELECT_QUEUE_HAS_ACCEL_PRIV) || defined(HAVE_SELECT_QUEUE_FALLBACK_T)
 u16 mlx4_en_select_queue(struct net_device *dev, struct sk_buff *skb,
+#ifdef HAVE_SELECT_QUEUE_FALLBACK_T
+#ifdef HAVE_SELECT_QUEUE_NET_DEVICE
 			 struct net_device *sb_dev,
+#else
+			 void *accel_priv,
+#endif /* HAVE_SELECT_QUEUE_NET_DEVICE */
 			 select_queue_fallback_t fallback)
+#else
+			 void *accel_priv)
+#endif
+#else /* NDO_SELECT_QUEUE_HAS_ACCEL_PRIV || HAVE_SELECT_QUEUE_FALLBACK_T */
+u16 mlx4_en_select_queue(struct net_device *dev, struct sk_buff *skb)
+#endif
 {
 	struct mlx4_en_priv *priv = netdev_priv(dev);
 	u16 rings_p_up = priv->num_tx_rings_p_up;
 
 	if (netdev_get_num_tc(dev))
+#ifdef HAVE_SELECT_QUEUE_FALLBACK_T_3_PARAMS
 		return fallback(dev, skb, NULL);
 
 	return fallback(dev, skb, NULL) % rings_p_up;
+#else
+		return fallback(dev, skb);
+
+	return fallback(dev, skb) % rings_p_up;
+#endif
 }
 
 static void mlx4_bf_copy(void __iomem *dst, const void *src,
@@ -754,7 +799,11 @@ static void mlx4_en_tx_write_desc(struct mlx4_en_tx_ring *ring,
 		/* Ensure new descriptor hits memory
 		 * before setting ownership of this descriptor to HW
 		 */
+#ifdef dma_wmb
 		dma_wmb();
+#else
+		wmb();
+#endif
 		tx_desc->ctrl.owner_opcode = op_own;
 
 		wmb();
@@ -769,12 +818,18 @@ static void mlx4_en_tx_write_desc(struct mlx4_en_tx_ring *ring,
 		/* Ensure new descriptor hits memory
 		 * before setting ownership of this descriptor to HW
 		 */
+#ifdef dma_wmb
 		dma_wmb();
+#else
+		wmb();
+#endif
 		tx_desc->ctrl.owner_opcode = op_own;
 		if (send_doorbell)
 			mlx4_en_xmit_doorbell(ring);
+#ifdef HAVE_SK_BUFF_XMIT_MORE
 		else
 			ring->xmit_more++;
+#endif
 	}
 }
 
@@ -805,7 +860,11 @@ static bool mlx4_en_build_dma_wqe(struct mlx4_en_priv *priv,
 
 		data->addr = cpu_to_be64(dma);
 		data->lkey = mr_key;
+#ifdef dma_wmb
 		dma_wmb();
+#else
+		wmb();
+#endif
 		data->byte_count = cpu_to_be32(byte_count);
 		--data;
 	}
@@ -822,7 +881,11 @@ static bool mlx4_en_build_dma_wqe(struct mlx4_en_priv *priv,
 
 		data->addr = cpu_to_be64(dma);
 		data->lkey = mr_key;
+#ifdef dma_wmb
 		dma_wmb();
+#else
+		wmb();
+#endif
 		data->byte_count = cpu_to_be32(byte_count);
 	}
 	/* tx completion can avoid cache line miss for common cases */
@@ -892,9 +955,12 @@ static inline netdev_tx_t __mlx4_en_xmit(struct sk_buff *skb,
 
 	bf_ok = ring->bf_enabled;
 	if (skb_vlan_tag_present(skb)) {
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 		u16 vlan_proto;
+#endif
 
 		qpn_vlan.vlan_tag = cpu_to_be16(skb_vlan_tag_get(skb));
+#ifdef HAVE_NETIF_F_HW_VLAN_STAG_RX
 		vlan_proto = be16_to_cpu(skb->vlan_proto);
 		if (vlan_proto == ETH_P_8021AD)
 			qpn_vlan.ins_vlan = MLX4_WQE_CTRL_INS_SVLAN;
@@ -902,10 +968,19 @@ static inline netdev_tx_t __mlx4_en_xmit(struct sk_buff *skb,
 			qpn_vlan.ins_vlan = MLX4_WQE_CTRL_INS_CVLAN;
 		else
 			qpn_vlan.ins_vlan = 0;
+#else
+		qpn_vlan.ins_vlan = MLX4_WQE_CTRL_INS_CVLAN;
+#endif
 		bf_ok = false;
 	}
 
+#ifdef HAVE_NETDEV_TXQ_BQL_PREFETCHW
 	netdev_txq_bql_enqueue_prefetchw(ring->tx_queue);
+#else
+#ifdef CONFIG_BQL
+	prefetchw(&ring->tx_queue->dql);
+#endif
+#endif
 
 	/* Track current inflight packets for performance analysis */
 	AVG_PERF_COUNTER(priv->pstats.inflight_avg,
@@ -962,8 +1037,13 @@ static inline netdev_tx_t __mlx4_en_xmit(struct sk_buff *skb,
 	 */
 	tx_info->ts_requested = 0;
 	if (unlikely(ring->hwtstamp_tx_type == HWTSTAMP_TX_ON &&
+#ifndef HAVE_SKB_SHARED_INFO_UNION_TX_FLAGS
 		     shinfo->tx_flags & SKBTX_HW_TSTAMP)) {
 		shinfo->tx_flags |= SKBTX_IN_PROGRESS;
+#else
+		     shinfo->tx_flags.flags & SKBTX_HW_TSTAMP)) {
+		shinfo->tx_flags.flags |= SKBTX_IN_PROGRESS;
+#endif
 		tx_info->ts_requested = 1;
 	}
 
@@ -971,11 +1051,16 @@ static inline netdev_tx_t __mlx4_en_xmit(struct sk_buff *skb,
 	 * whether LSO is used */
 	tx_desc->ctrl.srcrb_flags = priv->ctrl_flags;
 	if (likely(skb->ip_summed == CHECKSUM_PARTIAL)) {
+#ifdef HAVE_SK_BUFF_ENCAPSULATION
 		if (!skb->encapsulation)
 			tx_desc->ctrl.srcrb_flags |= cpu_to_be32(MLX4_WQE_CTRL_IP_CSUM |
 								 MLX4_WQE_CTRL_TCP_UDP_CSUM);
 		else
 			tx_desc->ctrl.srcrb_flags |= cpu_to_be32(MLX4_WQE_CTRL_IP_CSUM);
+#else
+		tx_desc->ctrl.srcrb_flags |= cpu_to_be32(MLX4_WQE_CTRL_IP_CSUM |
+							 MLX4_WQE_CTRL_TCP_UDP_CSUM);
+#endif
 		ring->tx_csum++;
 	}
 
@@ -1021,11 +1106,15 @@ static inline netdev_tx_t __mlx4_en_xmit(struct sk_buff *skb,
 		ring->packets++;
 	}
 	ring->bytes += tx_info->nr_bytes;
+#ifndef HAVE_NETDEV_TX_SEND_QUEUE
+	netdev_tx_sent_queue(ring->tx_queue, tx_info->nr_bytes);
+#endif
 	AVG_PERF_COUNTER(priv->pstats.tx_pktsz_avg, skb->len);
 
 	if (tx_info->inl)
 		build_inline_wqe(tx_desc, skb, shinfo, fragptr);
 
+#ifdef HAVE_SKB_INNER_NETWORK_HEADER
 	if (skb->encapsulation) {
 		union {
 			struct iphdr *v4;
@@ -1043,6 +1132,7 @@ static inline netdev_tx_t __mlx4_en_xmit(struct sk_buff *skb,
 		else
 			op_own |= cpu_to_be32(MLX4_WQE_CTRL_IIP);
 	}
+#endif
 
 	ring->prod += nr_txbb;
 
@@ -1059,9 +1149,17 @@ static inline netdev_tx_t __mlx4_en_xmit(struct sk_buff *skb,
 		ring->queue_stopped++;
 	}
 
+#ifdef HAVE_SK_BUFF_XMIT_MORE
+#ifdef HAVE_NETDEV_TX_SEND_QUEUE
 	send_doorbell = __netdev_tx_sent_queue(ring->tx_queue,
 					       tx_info->nr_bytes,
 					       skb->xmit_more);
+#else
+	send_doorbell = !skb->xmit_more || netif_xmit_stopped(ring->tx_queue);
+#endif
+#else
+	send_doorbell = true;
+#endif
 
 	real_size = (real_size / 16) & 0x3f;
 
@@ -1160,6 +1258,7 @@ void mlx4_en_init_tx_xdp_ring_descs(struct mlx4_en_priv *priv,
 	}
 }
 
+#ifdef HAVE_XDP_BUFF
 netdev_tx_t mlx4_en_xmit_frame(struct mlx4_en_rx_ring *rx_ring,
 			       struct mlx4_en_rx_alloc *frame,
 			       struct mlx4_en_priv *priv, unsigned int length,
@@ -1202,7 +1301,11 @@ netdev_tx_t mlx4_en_xmit_frame(struct mlx4_en_rx_ring *rx_ring,
 					 length, PCI_DMA_TODEVICE);
 
 	data->addr = cpu_to_be64(dma + frame->page_offset);
+#ifdef dma_wmb
 	dma_wmb();
+#else
+	wmb();
+#endif
 	data->byte_count = cpu_to_be32(length);
 
 	/* tx completion can avoid cache line miss for common cases */
@@ -1233,3 +1336,4 @@ tx_drop_count:
 tx_drop:
 	return NETDEV_TX_BUSY;
 }
+#endif
