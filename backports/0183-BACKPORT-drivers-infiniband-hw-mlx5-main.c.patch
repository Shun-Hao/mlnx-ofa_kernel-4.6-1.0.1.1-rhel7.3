From: Alaa Hleihel <alaa@mellanox.com>
Subject: [PATCH] BACKPORT: drivers/infiniband/hw/mlx5/main.c

Change-Id: I20b8bcc3b44bef40ee2e9f0012d58ee5450ba0a8
---
 drivers/infiniband/hw/mlx5/main.c | 46 +++++++++++++++++++++++++++++++++------
 1 file changed, 39 insertions(+), 7 deletions(-)

diff --git a/drivers/infiniband/hw/mlx5/main.c b/drivers/infiniband/hw/mlx5/main.c
index xxxxxxx..xxxxxxx 100644
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@ -53,7 +53,9 @@
 #include <linux/mlx5/port.h>
 #include <linux/mlx5/vport.h>
 #include <linux/mlx5/capi.h>
+#ifdef HAVE_MM_CONTEXT_ADD_COPRO
 #include <linux/mmu_context.h>
+#endif
 #include <linux/mlx5/fs.h>
 #include <linux/list.h>
 #include <rdma/ib_smi.h>
@@ -217,13 +219,17 @@ static int mlx5_netdev_event(struct notifier_block *this,
 	case NETDEV_CHANGE:
 	case NETDEV_UP:
 	case NETDEV_DOWN: {
+#ifdef HAVE_NETDEV_MASTER_UPPER_DEV_GET
 		struct net_device *lag_ndev = mlx5_lag_get_roce_netdev(mdev);
+#endif
 		struct net_device *upper = NULL;
 
+#ifdef HAVE_NETDEV_MASTER_UPPER_DEV_GET
 		if (lag_ndev) {
 			upper = netdev_master_upper_dev_get(lag_ndev);
 			dev_put(lag_ndev);
 		}
+#endif
 
 		if (ibdev->is_rep)
 			roce = mlx5_get_rep_roce(ibdev, ndev, &port_num);
@@ -612,10 +618,12 @@ static int set_roce_addr(struct mlx5_ib_dev *dev, u8 port_num,
 			 * device is vlan device, consider vlan id of
 			 * the lower vlan device for this gid entry.
 			 */
+#ifdef HAVE_NETDEV_WALK_ALL_LOWER_DEV_RCU
 			rcu_read_lock();
 			netdev_walk_all_lower_dev_rcu(attr->ndev,
 					get_lower_dev_vlan, &vlan_info);
 			rcu_read_unlock();
+#endif
 		}
 	}
 
@@ -1016,8 +1024,10 @@ int mlx5_ib_query_device(struct ib_device *ibdev,
 	if (MLX5_CAP_GEN(mdev, cd))
 		props->device_cap_flags |= IB_DEVICE_CROSS_CHANNEL;
 
+#ifdef HAVE_NDO_SET_VF_MAC
 	if (!mlx5_core_is_pf(mdev))
 		props->device_cap_flags |= IB_DEVICE_VIRTUAL_FUNCTION;
+#endif
 
 	if (mlx5_ib_port_link_layer(ibdev, 1) ==
 	    IB_LINK_LAYER_ETHERNET && raw_support) {
@@ -1834,7 +1844,9 @@ static int alloc_capi_context(struct mlx5_ib_dev *dev, struct mlx5_capi_context
 		goto out_mm;
 	}
 
+#ifdef HAVE_MM_CONTEXT_ADD_COPRO
 	mm_context_add_copro(cctx->mm);
+#endif
 	return 0;
 
 out_mm:
@@ -1852,7 +1864,9 @@ static int free_capi_context(struct mlx5_ib_dev *dev, struct mlx5_capi_context *
 	err = mlx5_core_destroy_pec(dev->mdev, cctx->pasid);
 	if (err)
 		mlx5_ib_warn(dev, "destroy pec failed\n");
+#ifdef HAVE_MM_CONTEXT_ADD_COPRO
 	mm_context_remove_copro(cctx->mm);
+#endif
 	mmdrop(cctx->mm);
 	return err;
 }
@@ -2169,9 +2183,11 @@ static int get_extended_index(unsigned long offset)
 }
 
 
+#if defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined (HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED)
 static void mlx5_ib_disassociate_ucontext(struct ib_ucontext *ibcontext)
 {
 }
+#endif
 
 static inline char *mmap_cmd2str(enum mlx5_ib_mmap_cmd cmd)
 {
@@ -2226,6 +2242,9 @@ static int uar_mmap(struct mlx5_ib_dev *dev, enum mlx5_ib_mmap_cmd cmd,
 	int dyn_uar = (cmd == MLX5_IB_MMAP_ALLOC_WC);
 	int max_valid_idx = dyn_uar ? bfregi->num_sys_pages :
 				bfregi->num_static_sys_pages;
+#if defined(CONFIG_X86) && !defined(HAVE_PAT_ENABLED_AS_FUNCTION)
+	pgprot_t tmp_prot = __pgprot(0);
+#endif
 
 	if (vma->vm_end - vma->vm_start != PAGE_SIZE)
 		return -EINVAL;
@@ -2246,7 +2265,11 @@ static int uar_mmap(struct mlx5_ib_dev *dev, enum mlx5_ib_mmap_cmd cmd,
 	case MLX5_IB_MMAP_ALLOC_WC:
 /* Some architectures don't support WC memory */
 #if defined(CONFIG_X86)
+#ifdef HAVE_PAT_ENABLED_AS_FUNCTION
 		if (!pat_enabled())
+#else
+		if (pgprot_val(pgprot_writecombine(tmp_prot)) == pgprot_val(pgprot_noncached(tmp_prot)))
+#endif
 			return -EPERM;
 #elif !(defined(CONFIG_PPC) || ((defined(CONFIG_ARM) || defined(CONFIG_ARM64)) && defined(CONFIG_MMU)))
 			return -EPERM;
@@ -6315,6 +6338,9 @@ static int mlx5_ib_stage_caps_init(struct mlx5_ib_dev *dev)
 	dev->ib_dev.exp_query_device	= mlx5_ib_exp_query_device;
 	dev->ib_dev.exp_query_mkey      = mlx5_ib_exp_query_mkey;
 	dev->ib_dev.exp_create_qp	= mlx5_ib_exp_create_qp;
+#ifdef HAVE_MM_STRUCT_FREE_AREA_CACHE
+	dev->ib_dev.exp_get_unmapped_area = mlx5_ib_exp_get_unmapped_area;
+#endif
 	dev->ib_dev.resize_cq		= mlx5_ib_resize_cq;
 	dev->ib_dev.destroy_cq		= mlx5_ib_destroy_cq;
 	dev->ib_dev.poll_cq		= mlx5_ib_poll_cq;
@@ -6337,14 +6363,20 @@ static int mlx5_ib_stage_caps_init(struct mlx5_ib_dev *dev)
 		dev->ib_dev.rdma_netdev_get_params = mlx5_ib_rn_get_params;
 
 	if (mlx5_core_is_pf(mdev)) {
-		dev->ib_dev.get_vf_config	= mlx5_ib_get_vf_config;
-		dev->ib_dev.set_vf_link_state	= mlx5_ib_set_vf_link_state;
-		dev->ib_dev.get_vf_stats	= mlx5_ib_get_vf_stats;
-		dev->ib_dev.set_vf_guid		= mlx5_ib_set_vf_guid;
-	}
-
+#ifdef HAVE_NDO_SET_VF_MAC
+       	dev->ib_dev.get_vf_config	= mlx5_ib_get_vf_config;
+#ifdef HAVE_LINKSTATE
+       	dev->ib_dev.set_vf_link_state	= mlx5_ib_set_vf_link_state;
+#endif
+       	dev->ib_dev.get_vf_stats	= mlx5_ib_get_vf_stats;
+#ifdef HAVE_IFLA_VF_IB_NODE_PORT_GUID
+       	dev->ib_dev.set_vf_guid		= mlx5_ib_set_vf_guid;
+#endif
+       }
+#endif
+#if defined(HAVE_PUT_TASK_STRUCT_EXPORTED) && defined (HAVE_GET_TASK_PID_EXPORTED) && defined(HAVE_GET_PID_TASK_EXPORTED)
 	dev->ib_dev.disassociate_ucontext = mlx5_ib_disassociate_ucontext;
-
+#endif
 	if (MLX5_CAP_GEN(mdev, nvmf_target_offload)) {
 		dev->ib_dev.create_nvmf_backend_ctrl  = mlx5_ib_create_nvmf_backend_ctrl;
 		dev->ib_dev.destroy_nvmf_backend_ctrl = mlx5_ib_destroy_nvmf_backend_ctrl;
