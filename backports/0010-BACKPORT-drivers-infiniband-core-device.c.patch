From: Talat Batheesh <talatb@mellanox.com>
Subject: [PATCH] BACKPORT: drivers/infiniband/core/device.c

Change-Id: Ia83a918aeae16044172b9eadd8920f961e9dd0dd
---
 drivers/infiniband/core/device.c | 128 +++++++++++++++++++++++++++++++++++++--
 1 file changed, 124 insertions(+), 4 deletions(-)

diff --git a/drivers/infiniband/core/device.c b/drivers/infiniband/core/device.c
index xxxxxxx..xxxxxxx 100644
--- a/drivers/infiniband/core/device.c
+++ b/drivers/infiniband/core/device.c
@@ -85,6 +85,7 @@ static LIST_HEAD(client_list);
 static DEFINE_MUTEX(device_mutex);
 static DECLARE_RWSEM(lists_rwsem);
 
+#ifdef HAVE_REGISTER_LSM_NOTIFIER
 static int ib_security_change(struct notifier_block *nb, unsigned long event,
 			      void *lsm_data);
 static void ib_policy_change_task(struct work_struct *work);
@@ -93,6 +94,7 @@ static DECLARE_WORK(ib_policy_change_work, ib_policy_change_task);
 static struct notifier_block ibdev_lsm_nb = {
 	.notifier_call = ib_security_change,
 };
+#endif
 
 static int ib_device_check_mandatory(struct ib_device *device)
 {
@@ -406,6 +408,7 @@ static int setup_port_pkey_list(struct ib_device *device)
 	return 0;
 }
 
+#ifdef HAVE_REGISTER_LSM_NOTIFIER
 static void ib_policy_change_task(struct work_struct *work)
 {
 	struct ib_device *dev;
@@ -440,6 +443,7 @@ static int ib_security_change(struct notifier_block *nb, unsigned long event,
 
 	return NOTIFY_OK;
 }
+#endif /* HAVE_REGISTER_LSM_NOTIFIER */
 
 /**
  *	__dev_new_index	-	allocate an device index
@@ -467,6 +471,7 @@ static u32 __dev_new_index(void)
 
 static void setup_dma_device(struct ib_device *device)
 {
+#ifdef HAVE_DEVICE_DMA_OPS
 	struct device *parent = device->dev.parent;
 
 	WARN_ON_ONCE(device->dma_device);
@@ -498,6 +503,15 @@ static void setup_dma_device(struct ib_device *device)
 		WARN_ON_ONCE(!parent);
 		device->dma_device = parent;
 	}
+#else /* HAVE_DEVICE_DMA_OPS */
+	WARN_ON_ONCE(!device->dev.parent && !device->dma_device);
+	WARN_ON_ONCE(device->dev.parent && device->dma_device
+		     && device->dev.parent != device->dma_device);
+	if (!device->dev.parent)
+		device->dev.parent = device->dma_device;
+	if (!device->dma_device)
+		device->dma_device = device->dev.parent;
+#endif /* HAVE_DEVICE_DMA_OPS */
 }
 
 static void cleanup_device(struct ib_device *device)
@@ -588,19 +602,24 @@ int ib_register_device(struct ib_device *device, const char *name,
 	}
 	strlcpy(device->name, dev_name(&device->dev), IB_DEVICE_NAME_MAX);
 
+#if !defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
+	mutex_init(&device->skprio2up.lock);
+#endif
+
 	ret = setup_device(device);
 	if (ret)
 		goto out;
 
 	device->index = __dev_new_index();
 
+#ifdef HAVE_CGROUP_RDMA_H
 	ret = ib_device_register_rdmacg(device);
 	if (ret) {
 		dev_warn(&device->dev,
 			 "Couldn't register device with rdma cgroup\n");
 		goto dev_cleanup;
 	}
-
+#endif
 	ret = ib_device_register_sysfs(device, port_callback);
 	if (ret) {
 		dev_warn(&device->dev,
@@ -621,8 +640,10 @@ int ib_register_device(struct ib_device *device, const char *name,
 	return 0;
 
 cg_cleanup:
+#ifdef HAVE_CGROUP_RDMA_H
 	ib_device_unregister_rdmacg(device);
 dev_cleanup:
+#endif
 	cleanup_device(device);
 out:
 	mutex_unlock(&device_mutex);
@@ -658,7 +679,9 @@ void ib_unregister_device(struct ib_device *device)
 	up_read(&lists_rwsem);
 
 	ib_device_unregister_sysfs(device);
+#ifdef HAVE_CGROUP_RDMA_H
 	ib_device_unregister_rdmacg(device);
+#endif
 
 	mutex_unlock(&device_mutex);
 
@@ -1114,6 +1137,46 @@ int ib_find_gid(struct ib_device *device, union ib_gid *gid,
 }
 EXPORT_SYMBOL(ib_find_gid);
 
+#if !defined(HAVE_VLAN_DEV_GET_EGRESS_QOS_MASK)
+int ib_set_skprio2up(struct ib_device *device,
+		     u8 port_num, u8 prio, u8 up)
+{
+	if (prio >= NUM_SKPRIO ||
+	    up >= NUM_UP ||
+	    port_num > MAX_PORTS || port_num == 0)
+		return -EINVAL;
+
+	if (rdma_port_get_link_layer(device, port_num) !=
+			IB_LINK_LAYER_ETHERNET)
+		return -ENOTSUPP;
+
+	mutex_lock(&device->skprio2up.lock);
+	device->skprio2up.map[port_num - 1][prio] = up;
+	mutex_unlock(&device->skprio2up.lock);
+	return 0;
+}
+EXPORT_SYMBOL(ib_set_skprio2up);
+
+int ib_get_skprio2up(struct ib_device *device,
+		     u8 port_num, u8 prio, u8 *up)
+{
+	if (prio >= NUM_SKPRIO ||
+	    !up ||
+	    port_num > MAX_PORTS || port_num == 0)
+		return -EINVAL;
+
+	if (rdma_port_get_link_layer(device, port_num) !=
+			IB_LINK_LAYER_ETHERNET)
+		return -ENOTSUPP;
+
+	mutex_lock(&device->skprio2up.lock);
+	*up = device->skprio2up.map[port_num - 1][prio];
+	mutex_unlock(&device->skprio2up.lock);
+	return 0;
+}
+EXPORT_SYMBOL(ib_get_skprio2up);
+#endif
+
 /**
  * ib_find_pkey - Returns the PKey table index where a specified
  *   PKey value occurs.
@@ -1204,11 +1267,19 @@ static const struct rdma_nl_cbs ibnl_ls_cb_table[RDMA_NL_LS_NUM_OPS] = {
 		.flags = RDMA_NL_ADMIN_PERM,
 	},
 	[RDMA_NL_LS_OP_SET_TIMEOUT] = {
+#ifdef HAVE_NETLINK_EXT_ACK
 		.doit = ib_nl_handle_set_timeout,
+#else
+		.dump = ib_nl_handle_set_timeout,
+#endif
 		.flags = RDMA_NL_ADMIN_PERM,
 	},
 	[RDMA_NL_LS_OP_IP_RESOLVE] = {
+#ifdef HAVE_NETLINK_EXT_ACK
 		.doit = ib_nl_handle_ip_res_resp,
+#else
+		.dump = ib_nl_handle_ip_res_resp,
+#endif
 		.flags = RDMA_NL_ADMIN_PERM,
 	},
 };
@@ -1221,17 +1292,60 @@ static int __init ib_core_init(void)
 	if (!ib_wq)
 		return -ENOMEM;
 
+#if defined(HAVE_ALLOC_WORKQUEUE)
 	ib_comp_wq = alloc_workqueue("ib-comp-wq",
-			WQ_HIGHPRI | WQ_MEM_RECLAIM | WQ_SYSFS, 0);
+			0
+#if defined(HAVE_WQ_HIGHPRI)
+			| WQ_HIGHPRI
+#endif
+#if defined(HAVE_WQ_MEM_RECLAIM)
+			| WQ_MEM_RECLAIM
+#endif
+#if defined(HAVE_WQ_SYSFS)
+			| WQ_SYSFS
+#endif
+#if defined(HAVE_WQ_NON_REENTRANT)
+			| WQ_NON_REENTRANT
+#endif
+			, 0);
+#else /* HAVE_ALLOC_WORKQUEUE */
+	/* For older kernels that do not have WQ_NON_REENTRANT and
+	 * alloc_workqueue
+	 */
+	ib_comp_wq = create_singlethread_workqueue("ib-comp-wq");
+#endif /* HAVE_ALLOC_WORKQUEUE */
 	if (!ib_comp_wq) {
 		ret = -ENOMEM;
 		goto err;
 	}
 
+#if defined(HAVE_ALLOC_WORKQUEUE)
 	ib_comp_unbound_wq =
 		alloc_workqueue("ib-comp-unb-wq",
-				WQ_UNBOUND | WQ_HIGHPRI | WQ_MEM_RECLAIM |
-				WQ_SYSFS, WQ_UNBOUND_MAX_ACTIVE);
+			0
+#if defined(HAVE_WQ_UNBOUND_MAX_ACTIVE)
+			| WQ_UNBOUND | WQ_UNBOUND_MAX_ACTIVE
+#elif defined(HAVE_WQ_UNBOUND)
+			| WQ_UNBOUND
+#endif
+#if defined(HAVE_WQ_HIGHPRI)
+			| WQ_HIGHPRI
+#endif
+#if defined(HAVE_WQ_MEM_RECLAIM)
+			| WQ_MEM_RECLAIM
+#endif
+#if defined(HAVE_WQ_SYSFS)
+			| WQ_SYSFS
+#endif
+#if defined(HAVE_WQ_NON_REENTRANT)
+			| WQ_NON_REENTRANT
+#endif
+			, 0);
+#else /* HAVE_ALLOC_WORKQUEUE */
+	/* For older kernels that do not have alloc_workqueue
+	 */
+	ib_comp_unbound_wq = create_singlethread_workqueue("ib-comp-unb-wq");
+#endif /* HAVE_ALLOC_WORKQUEUE */
 	if (!ib_comp_unbound_wq) {
 		ret = -ENOMEM;
 		goto err_comp;
@@ -1267,11 +1381,13 @@ static int __init ib_core_init(void)
 		goto err_mad;
 	}
 
+#ifdef HAVE_REGISTER_LSM_NOTIFIER
 	ret = register_lsm_notifier(&ibdev_lsm_nb);
 	if (ret) {
 		pr_warn("Couldn't register LSM notifier. ret %d\n", ret);
 		goto err_sa;
 	}
+#endif
 
 	nldev_init();
 	rdma_nl_register(RDMA_NL_LS, ibnl_ls_cb_table);
@@ -1279,8 +1395,10 @@ static int __init ib_core_init(void)
 
 	return 0;
 
+#ifdef HAVE_REGISTER_LSM_NOTIFIER
 err_sa:
 	ib_sa_cleanup();
+#endif
 err_mad:
 	ib_mad_cleanup();
 err_addr:
@@ -1303,7 +1421,9 @@ static void __exit ib_core_cleanup(void)
 	roce_gid_mgmt_cleanup();
 	nldev_exit();
 	rdma_nl_unregister(RDMA_NL_LS);
+#ifdef HAVE_REGISTER_LSM_NOTIFIER
 	unregister_lsm_notifier(&ibdev_lsm_nb);
+#endif
 	ib_sa_cleanup();
 	ib_mad_cleanup();
 	addr_cleanup();
