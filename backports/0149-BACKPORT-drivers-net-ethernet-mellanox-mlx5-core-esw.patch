From: Chris Mi <chrism@mellanox.com>
Subject: [PATCH] BACKPORT:
 drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c

Change-Id: I5b94affcb17bb997531e064e4ac70088afbd685d
---
 .../ethernet/mellanox/mlx5/core/eswitch_offloads.c | 144 ++++++++++++++++++---
 1 file changed, 127 insertions(+), 17 deletions(-)

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c b/drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
index xxxxxxx..xxxxxxx 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
@@ -117,7 +117,11 @@ u16 mlx5_eswitch_get_prio_range(struct mlx5_eswitch *esw)
 	if (esw->fdb_table.flags & ESW_FDB_CHAINS_AND_PRIOS_SUPPORTED)
 		return FDB_MAX_PRIO;
 
+#if defined(HAVE_TC_CLS_FLOWER_OFFLOAD_COMMON) && defined (HAVE_IS_TCF_GACT_GOTO_CHAIN)
 	return 1;
+#else
+	return U16_MAX;
+#endif
 }
 
 struct mlx5_flow_handle *
@@ -198,12 +202,15 @@ mlx5_eswitch_add_offloaded_rule(struct mlx5_eswitch *esw,
 				 source_eswitch_owner_vhca_id);
 
 	spec->match_criteria_enable = MLX5_MATCH_MISC_PARAMETERS;
+#ifdef HAVE_TCF_TUNNEL_INFO
 	if (flow_act.action & MLX5_FLOW_CONTEXT_ACTION_DECAP) {
 		if (attr->tunnel_match_level != MLX5_MATCH_NONE)
 			spec->match_criteria_enable |= MLX5_MATCH_OUTER_HEADERS;
 		if (attr->match_level != MLX5_MATCH_NONE)
 			spec->match_criteria_enable |= MLX5_MATCH_INNER_HEADERS;
-	} else if (attr->match_level != MLX5_MATCH_NONE) {
+	} else
+#endif
+	if (attr->match_level != MLX5_MATCH_NONE) {
 		spec->match_criteria_enable |= MLX5_MATCH_OUTER_HEADERS;
 	}
 
@@ -1583,10 +1590,15 @@ disable_roce:
 	return err;
 }
 
-static int esw_offloads_start(struct mlx5_eswitch *esw,
-			      struct netlink_ext_ack *extack)
+static int esw_offloads_start(struct mlx5_eswitch *esw
+#ifdef HAVE_DEVLINK_ESWITCH_MODE_SET_EXTACK
+			      , struct netlink_ext_ack *extack
+#endif
+			     )
 {
+#ifdef HAVE_DEVLINK_ESWITCH_MODE_SET_EXTACK
 	esw->handler.extack = extack;
+#endif
 	return schedule_work(&esw->handler.start_handler) != true;
 }
 
@@ -1597,12 +1609,19 @@ void esw_offloads_start_handler(struct work_struct *work)
 	struct mlx5_eswitch *esw =
 	       container_of(handler, struct mlx5_eswitch, handler);
 	int err, err1, num_vfs = esw->dev->priv.sriov.num_vfs;
+#ifdef HAVE_DEVLINK_ESWITCH_MODE_SET_EXTACK
 	struct netlink_ext_ack *extack = handler->extack;
+#endif
 
 	if (esw->mode != SRIOV_LEGACY &&
 	    !mlx5_core_is_ecpf_esw_manager(esw->dev)) {
+#ifdef HAVE_DEVLINK_ESWITCH_MODE_SET_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
 				   "Can't set offloads mode, SRIOV legacy not enabled");
+#else
+		esw_warn(esw->dev,
+			 "Can't set offloads mode, SRIOV legacy not enabled\n");
+#endif
 		atomic_set(&esw->handler.in_progress, 0);
 		return;
 	}
@@ -1610,12 +1629,21 @@ void esw_offloads_start_handler(struct work_struct *work)
 	mlx5_eswitch_disable_sriov(esw);
 	err = mlx5_eswitch_enable_sriov(esw, num_vfs, SRIOV_OFFLOADS);
 	if (err) {
+#ifdef HAVE_DEVLINK_ESWITCH_MODE_SET_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
 				   "Failed setting eswitch to offloads");
+#else
+		esw_warn(esw->dev, "Failed setting eswitch to offloads\n");
+#endif
 		err1 = mlx5_eswitch_enable_sriov(esw, num_vfs, SRIOV_LEGACY);
 		if (err1) {
+#ifdef HAVE_DEVLINK_ESWITCH_MODE_SET_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "Failed setting eswitch back to legacy");
+#else
+			esw_warn(esw->dev,
+				 "Failed setting eswitch back to legacy\n");
+#endif
 		}
 	}
 	if (esw->offloads.inline_mode == MLX5_INLINE_MODE_NONE) {
@@ -1623,8 +1651,13 @@ void esw_offloads_start_handler(struct work_struct *work)
 						 num_vfs,
 						 &esw->offloads.inline_mode)) {
 			esw->offloads.inline_mode = MLX5_INLINE_MODE_L2;
+#ifdef HAVE_DEVLINK_ESWITCH_MODE_SET_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "Inline mode is different between vports");
+#else
+			esw_warn(esw->dev,
+				 "Inline mode is different between vports\n");
+#endif
 		}
 	}
 	atomic_set(&esw->handler.in_progress, 0);
@@ -2092,10 +2125,15 @@ err_reps:
 	return err;
 }
 
-static int esw_offloads_stop(struct mlx5_eswitch *esw,
-			     struct netlink_ext_ack *extack)
+static int esw_offloads_stop(struct mlx5_eswitch *esw
+#ifdef HAVE_DEVLINK_ESWITCH_MODE_SET_EXTACK
+			     , struct netlink_ext_ack *extack
+#endif
+			    )
 {
+#ifdef HAVE_DEVLINK_ESWITCH_MODE_SET_EXTACK
 	esw->handler.extack = extack;
+#endif
 	return schedule_work(&esw->handler.stop_handler) != true;
 }
 
@@ -2106,16 +2144,27 @@ void esw_offloads_stop_handler(struct work_struct *work)
 	struct mlx5_eswitch *esw =
 		container_of(handler, struct mlx5_eswitch, handler);
 	int err, err1, num_vfs = esw->dev->priv.sriov.num_vfs;
+#ifdef HAVE_DEVLINK_ESWITCH_MODE_SET_EXTACK
 	struct netlink_ext_ack *extack = handler->extack;
+#endif
 
 	mlx5_eswitch_disable_sriov(esw);
 	err = mlx5_eswitch_enable_sriov(esw, num_vfs, SRIOV_LEGACY);
 	if (err) {
+#ifdef HAVE_DEVLINK_ESWITCH_MODE_SET_EXTACK
 		NL_SET_ERR_MSG_MOD(extack, "Failed setting eswitch to legacy");
+#else
+		esw_warn(esw->dev, "Failed setting eswitch to legacy\n");
+#endif
 		err1 = mlx5_eswitch_enable_sriov(esw, num_vfs, SRIOV_OFFLOADS);
 		if (err1) {
+#ifdef HAVE_DEVLINK_ESWITCH_MODE_SET_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "Failed setting eswitch back to offloads");
+#else
+			esw_warn(esw->dev,
+				 "Failed setting eswitch back to offloads\n");
+#endif
 		}
 	}
 
@@ -2238,8 +2287,34 @@ static int mlx5_devlink_eswitch_check(struct devlink *devlink)
 	return 0;
 }
 
-int mlx5_devlink_eswitch_mode_set(struct devlink *devlink, u16 mode,
-				  struct netlink_ext_ack *extack)
+DEFINE_MUTEX(devlink_lock);
+#define DEVLINK_LOCK(func, type1, arg1)\
+func ## _locked(struct devlink *devlink, type1 arg1);\
+int func(struct devlink *devlink, type1 arg1) {\
+	int ret;\
+	mutex_lock(&devlink_lock);\
+	ret = func ## _locked(devlink, arg1);\
+	mutex_unlock(&devlink_lock);\
+	return ret;\
+}\
+int func ## _locked(struct devlink *devlink, type1 arg1)
+
+#define DEVLINK_LOCK_2(func, type1, arg1, type2, arg2)\
+func ## _locked(struct devlink *devlink, type1 arg1, type2 arg2);\
+int func(struct devlink *devlink, type1 arg1, type2 arg2) {\
+	int ret;\
+	mutex_lock(&devlink_lock);\
+	ret = func ## _locked(devlink, arg1, arg2);\
+	mutex_unlock(&devlink_lock);\
+	return ret;\
+}\
+int func ## _locked(struct devlink *devlink, type1 arg1, type2 arg2)
+#ifdef HAVE_DEVLINK_ESWITCH_MODE_SET_EXTACK
+int DEVLINK_LOCK_2(mlx5_devlink_eswitch_mode_set, u16, mode,
+				  struct netlink_ext_ack *, extack)
+#else
+int DEVLINK_LOCK(mlx5_devlink_eswitch_mode_set, u16, mode)
+#endif
 {
 	struct mlx5_core_dev *dev = devlink_priv(devlink);
 	struct mlx5_eswitch *esw = dev->priv.eswitch;
@@ -2262,15 +2337,23 @@ int mlx5_devlink_eswitch_mode_set(struct devlink *devlink, u16 mode,
 		return -EBUSY;
 
 	if (mode == DEVLINK_ESWITCH_MODE_SWITCHDEV)
-		return esw_offloads_start(esw, extack);
+		return esw_offloads_start(dev->priv.eswitch
+#ifdef HAVE_DEVLINK_ESWITCH_MODE_SET_EXTACK
+					  , extack
+#endif
+					 );
 	else if (mode == DEVLINK_ESWITCH_MODE_LEGACY)
-		return esw_offloads_stop(esw, extack);
+		return esw_offloads_stop(dev->priv.eswitch
+#ifdef HAVE_DEVLINK_ESWITCH_MODE_SET_EXTACK
+					 , extack
+#endif
+					);
 
 	atomic_dec(&esw->handler.in_progress);
 	return -EINVAL;
 }
 
-int mlx5_devlink_eswitch_mode_get(struct devlink *devlink, u16 *mode)
+int DEVLINK_LOCK(mlx5_devlink_eswitch_mode_get, u16 *, mode)
 {
 	struct mlx5_core_dev *dev = devlink_priv(devlink);
 	int err;
@@ -2282,8 +2365,11 @@ int mlx5_devlink_eswitch_mode_get(struct devlink *devlink, u16 *mode)
 	return esw_mode_to_devlink(dev->priv.eswitch->mode, mode);
 }
 
-int mlx5_devlink_eswitch_inline_mode_set(struct devlink *devlink, u8 mode,
-					 struct netlink_ext_ack *extack)
+int mlx5_devlink_eswitch_inline_mode_set(struct devlink *devlink, u8 mode
+#ifdef HAVE_DEVLINK_ESWITCH_MODE_SET_EXTACK
+					 , struct netlink_ext_ack *extack
+#endif
+					)
 {
 	struct mlx5_core_dev *dev = devlink_priv(devlink);
 	struct mlx5_eswitch *esw = dev->priv.eswitch;
@@ -2300,15 +2386,23 @@ int mlx5_devlink_eswitch_inline_mode_set(struct devlink *devlink, u8 mode,
 			return 0;
 		/* fall through */
 	case MLX5_CAP_INLINE_MODE_L2:
+#ifdef HAVE_DEVLINK_ESWITCH_MODE_SET_EXTACK
 		NL_SET_ERR_MSG_MOD(extack, "Inline mode can't be set");
+#else
+		esw_warn(dev, "Inline mode can't be set\n");
+#endif
 		return -EOPNOTSUPP;
 	case MLX5_CAP_INLINE_MODE_VPORT_CONTEXT:
 		break;
 	}
 
 	if (atomic64_read(&esw->offloads.num_flows) > 0) {
+#ifdef HAVE_DEVLINK_ESWITCH_MODE_SET_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
 				   "Can't set inline mode when flows are configured");
+#else
+		esw_warn(dev, "Can't set inline mode when flows are configured\n");
+#endif
 		return -EOPNOTSUPP;
 	}
 
@@ -2319,8 +2413,12 @@ int mlx5_devlink_eswitch_inline_mode_set(struct devlink *devlink, u8 mode,
 	mlx5_esw_for_each_vf_vport(esw, vport, esw->enabled_vports) {
 		err = mlx5_modify_nic_vport_min_inline(dev, vport, mlx5_mode);
 		if (err) {
+#ifdef HAVE_DEVLINK_ESWITCH_MODE_SET_EXTACK
 			NL_SET_ERR_MSG_MOD(extack,
 					   "Failed to set min inline on vport");
+#else
+			esw_warn(dev, "Failed to set min inline on vport\n");
+#endif
 			goto revert_inline_mode;
 		}
 	}
@@ -2337,7 +2435,7 @@ out:
 	return err;
 }
 
-int mlx5_devlink_eswitch_inline_mode_get(struct devlink *devlink, u8 *mode)
+int DEVLINK_LOCK(mlx5_devlink_eswitch_inline_mode_get, u8 *, mode)
 {
 	struct mlx5_core_dev *dev = devlink_priv(devlink);
 	struct mlx5_eswitch *esw = dev->priv.eswitch;
@@ -2352,7 +2450,7 @@ int mlx5_devlink_eswitch_inline_mode_get(struct devlink *devlink, u8 *mode)
 
 int mlx5_eswitch_inline_mode_get(struct mlx5_eswitch *esw, int nvfs, u8 *mode)
 {
-	u8 prev_mlx5_mode, mlx5_mode = MLX5_INLINE_MODE_L2;
+	u8 prev_mlx5_mode = 0, mlx5_mode = MLX5_INLINE_MODE_L2;
 	struct mlx5_core_dev *dev = esw->dev;
 	int vport;
 
@@ -2386,8 +2484,11 @@ out:
 	return 0;
 }
 
-int mlx5_devlink_eswitch_encap_mode_set(struct devlink *devlink, u8 encap,
-					struct netlink_ext_ack *extack)
+int mlx5_devlink_eswitch_encap_mode_set(struct devlink *devlink, u8 encap
+#ifdef HAVE_DEVLINK_ESWITCH_MODE_SET_EXTACK
+					, struct netlink_ext_ack *extack
+#endif
+					)
 {
 	struct mlx5_core_dev *dev = devlink_priv(devlink);
 	struct mlx5_eswitch *esw = dev->priv.eswitch;
@@ -2414,8 +2515,13 @@ int mlx5_devlink_eswitch_encap_mode_set(struct devlink *devlink, u8 encap,
 		return 0;
 
 	if (atomic64_read(&esw->offloads.num_flows) > 0) {
+#ifdef HAVE_DEVLINK_ESWITCH_MODE_SET_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
 				   "Can't set encapsulation when flows are configured");
+#else
+		esw_warn(esw->dev,
+			 "Can't set encapsulation when flows are configured\n");
+#endif
 		return -EOPNOTSUPP;
 	}
 
@@ -2426,8 +2532,12 @@ int mlx5_devlink_eswitch_encap_mode_set(struct devlink *devlink, u8 encap,
 	err = esw_create_offloads_fdb_tables(esw, esw->nvports);
 
 	if (err) {
+#ifdef HAVE_DEVLINK_ESWITCH_MODE_SET_EXTACK
 		NL_SET_ERR_MSG_MOD(extack,
 				   "Failed re-creating fast FDB table");
+#else
+		esw_warn(esw->dev, "Failed re-creating fast FDB table\n");
+#endif
 		esw->offloads.encap = !encap;
 		(void)esw_create_offloads_fdb_tables(esw, esw->nvports);
 	}
@@ -2435,7 +2545,7 @@ int mlx5_devlink_eswitch_encap_mode_set(struct devlink *devlink, u8 encap,
 	return err;
 }
 
-int mlx5_devlink_eswitch_encap_mode_get(struct devlink *devlink, u8 *encap)
+int DEVLINK_LOCK(mlx5_devlink_eswitch_encap_mode_get, u8 *, encap)
 {
 	struct mlx5_core_dev *dev = devlink_priv(devlink);
 	struct mlx5_eswitch *esw = dev->priv.eswitch;
