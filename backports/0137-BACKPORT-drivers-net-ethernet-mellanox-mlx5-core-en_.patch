From: Mikhael Goikhman <migo@mellanox.com>
Subject: [PATCH] BACKPORT: drivers/net/ethernet/mellanox/mlx5/core/en_rx.c

Change-Id: Iac87d0f6bc1116dd2254d1fa6dd50f0d43c52716
---
 drivers/net/ethernet/mellanox/mlx5/core/en_rx.c | 270 +++++++++++++++++++++++-
 1 file changed, 268 insertions(+), 2 deletions(-)

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
index xxxxxxx..xxxxxxx 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@ -35,7 +35,9 @@
 #include <linux/ipv6.h>
 #include <linux/tcp.h>
 #include <net/ip6_checksum.h>
+#ifdef HAVE_NET_PAGE_POOL_H
 #include <net/page_pool.h>
+#endif
 #include <net/inet_ecn.h>
 #include "en.h"
 #include "en_tc.h"
@@ -275,7 +277,11 @@ static inline bool mlx5e_rx_cache_extend(struct mlx5e_rq *rq)
 
 static inline bool mlx5e_page_is_reserved(struct page *page)
 {
+#ifdef HAVE_PAGE_IS_PFMEMALLOC
 	return page_is_pfmemalloc(page) || page_to_nid(page) != numa_mem_id();
+#else
+	return page_to_nid(page) != numa_node_id();
+#endif
 }
 
 static inline bool mlx5e_rx_cache_put(struct mlx5e_rq *rq,
@@ -331,7 +337,11 @@ static inline int mlx5e_page_alloc_mapped(struct mlx5e_rq *rq,
 					  struct mlx5e_dma_info *dma_info)
 {
 	if (!mlx5e_rx_cache_get(rq, dma_info)) {
+#ifdef HAVE_NET_PAGE_POOL_H
 		dma_info->page = page_pool_dev_alloc_pages(rq->page_pool);
+#else
+		dma_info->page = dev_alloc_page();
+#endif
 		if (unlikely(!dma_info->page))
 			return -ENOMEM;
 		rq->stats->cache_alloc++;
@@ -357,6 +367,7 @@ void mlx5e_page_release(struct mlx5e_rq *rq, struct mlx5e_dma_info *dma_info,
 			bool recycle)
 {
 	mlx5e_page_dma_unmap(rq, dma_info);
+#ifdef HAVE_NET_PAGE_POOL_H
 	if (likely(recycle)) {
 		if (mlx5e_rx_cache_put(rq, dma_info))
 			return;
@@ -365,6 +376,12 @@ void mlx5e_page_release(struct mlx5e_rq *rq, struct mlx5e_dma_info *dma_info,
 	} else {
 		put_page(dma_info->page);
 	}
+#else
+	if (likely(recycle) && mlx5e_rx_cache_put(rq, dma_info))
+		return;
+
+	put_page(dma_info->page);
+#endif
 }
 
 static inline int mlx5e_get_rx_frag(struct mlx5e_rq *rq,
@@ -508,6 +525,7 @@ mlx5e_copy_skb_header_mpwqe(struct device *pdev,
 static void
 mlx5e_free_rx_mpwqe(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi, bool recycle)
 {
+#ifdef HAVE_XDP_BUFF
 	const bool no_xdp_xmit =
 		bitmap_empty(wi->xdp_xmit_bitmap, MLX5_MPWRQ_PAGES_PER_WQE);
 	struct mlx5e_dma_info *dma_info = wi->umr.dma_info;
@@ -516,6 +534,13 @@ mlx5e_free_rx_mpwqe(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi, bool recycle
 	for (i = 0; i < MLX5_MPWRQ_PAGES_PER_WQE; i++)
 		if (no_xdp_xmit || !test_bit(i, wi->xdp_xmit_bitmap))
 			mlx5e_page_release(rq, &dma_info[i], recycle);
+#else
+	struct mlx5e_dma_info *dma_info = &wi->umr.dma_info[0];
+	int i;
+
+	for (i = 0; i < MLX5_MPWRQ_PAGES_PER_WQE; i++, dma_info++)
+		mlx5e_page_release(rq, dma_info, true);
+#endif
 }
 
 static void mlx5e_post_rx_mpwqe(struct mlx5e_rq *rq)
@@ -528,7 +553,11 @@ static void mlx5e_post_rx_mpwqe(struct mlx5e_rq *rq)
 	mlx5_wq_ll_push(wq, be16_to_cpu(wqe->next.next_wqe_index));
 
 	/* ensure wqes are visible to device before updating doorbell record */
+#ifdef dma_wmb
 	dma_wmb();
+#else
+	wmb();
+#endif
 
 	mlx5_wq_ll_update_db_record(wq);
 
@@ -586,7 +615,9 @@ static int mlx5e_alloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix)
 		umr_wqe->inline_mtts[i].ptag = cpu_to_be64(dma_info->addr | MLX5_EN_WR);
 	}
 
+#ifdef HAVE_XDP_BUFF
 	bitmap_zero(wi->xdp_xmit_bitmap, MLX5_MPWRQ_PAGES_PER_WQE);
+#endif
 	wi->consumed_strides = 0;
 
 	rq->mpwqe.umr_in_progress = true;
@@ -646,7 +677,11 @@ bool mlx5e_post_rx_wqes(struct mlx5e_rq *rq)
 	} while (mlx5_wq_cyc_missing(wq) >= wqe_bulk);
 
 	/* ensure wqes are visible to device before updating doorbell record */
+#ifdef dma_wmb
 	dma_wmb();
+#else
+	wmb();
+#endif
 
 	mlx5_wq_cyc_update_db_record(wq);
 
@@ -790,15 +825,21 @@ static void mlx5e_lro_update_hdr(struct sk_buff *skb, struct mlx5_cqe64 *cqe,
 	}
 }
 
+#ifdef HAVE_NETIF_F_RXHASH
 static inline void mlx5e_skb_set_hash(struct mlx5_cqe64 *cqe,
 				      struct sk_buff *skb)
 {
+#ifdef HAVE_SKB_SET_HASH
 	u8 cht = cqe->rss_hash_type;
 	int ht = (cht & CQE_RSS_HTYPE_L4) ? PKT_HASH_TYPE_L4 :
 		 (cht & CQE_RSS_HTYPE_IP) ? PKT_HASH_TYPE_L3 :
 					    PKT_HASH_TYPE_NONE;
 	skb_set_hash(skb, be32_to_cpu(cqe->rss_hash_result), ht);
+#else
+	skb->rxhash = be32_to_cpu(cqe->rss_hash_result);
+#endif
 }
+#endif
 
 static inline bool is_last_ethertype_ip(struct sk_buff *skb, int *network_depth,
 					__be16 *proto)
@@ -820,11 +861,16 @@ static inline void mlx5e_enable_ecn(struct mlx5e_rq *rq, struct sk_buff *skb)
 
 	ip = skb->data + network_depth;
 	rc = ((proto == htons(ETH_P_IP)) ? IP_ECN_set_ce((struct iphdr *)ip) :
+#ifdef HAVE_IP6_SET_CE_2_PARAMS
 					 IP6_ECN_set_ce(skb, (struct ipv6hdr *)ip));
+#else
+					 IP6_ECN_set_ce((struct ipv6hdr *)ip));
+#endif
 
 	rq->stats->ecn_mark += !!rc;
 }
 
+#ifdef HAVE_NETIF_F_RXFCS
 static u32 mlx5e_get_fcs(const struct sk_buff *skb)
 {
 	const void *fcs_bytes;
@@ -835,6 +881,7 @@ static u32 mlx5e_get_fcs(const struct sk_buff *skb)
 
 	return __get_unaligned_cpu32(fcs_bytes);
 }
+#endif
 
 static u8 get_ip_proto(struct sk_buff *skb, int network_depth, __be16 proto)
 {
@@ -893,10 +940,12 @@ static inline void mlx5e_handle_csum(struct net_device *netdev,
 			skb->csum = csum_partial(skb->data + ETH_HLEN,
 						 network_depth - ETH_HLEN,
 						 skb->csum);
+#ifdef HAVE_NETIF_F_RXFCS
 		if (unlikely(netdev->features & NETIF_F_RXFCS))
 			skb->csum = csum_block_add(skb->csum,
 						   (__force __wsum)mlx5e_get_fcs(skb),
 						   skb->len - ETH_FCS_LEN);
+#endif
 		stats->csum_complete++;
 		return;
 	}
@@ -907,8 +956,12 @@ csum_unnecessary:
 		    (get_cqe_l4_hdr_type(cqe) == CQE_L4_HDR_TYPE_NONE)))) {
 		skb->ip_summed = CHECKSUM_UNNECESSARY;
 		if (cqe_is_tunneled(cqe)) {
+#ifdef HAVE_SK_BUFF_CSUM_LEVEL
 			skb->csum_level = 1;
+#endif
+#ifdef HAVE_SK_BUFF_ENCAPSULATION
 			skb->encapsulation = 1;
+#endif
 			stats->csum_unnecessary_inner++;
 			return;
 		}
@@ -930,10 +983,14 @@ static inline void mlx5e_build_rx_skb(struct mlx5_cqe64 *cqe,
 	u8 lro_num_seg = be32_to_cpu(cqe->srqn) >> 24;
 	struct mlx5e_rq_stats *stats = rq->stats;
 	struct net_device *netdev = rq->netdev;
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct mlx5e_priv *priv = netdev_priv(netdev);
+	u8 l4_hdr_type;
+#endif
 
 	skb->mac_len = ETH_HLEN;
 
-#ifdef CONFIG_MLX5_EN_TLS
+#if defined(CONFIG_MLX5_EN_TLS) && defined(HAVE_TLS_OFFLOAD_RX_RSYNC_REQUEST)
 	mlx5e_tls_handle_rx_skb(netdev, skb, &cqe_bcnt);
 #endif
 
@@ -946,6 +1003,16 @@ static inline void mlx5e_build_rx_skb(struct mlx5_cqe64 *cqe,
 		stats->packets += lro_num_seg - 1;
 		stats->lro_packets++;
 		stats->lro_bytes += cqe_bcnt;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0)
+		/* Flush GRO to avoid OOO packets, since GSO bypasses the
+		 * GRO queue. This was fixed in dev_gro_receive() in kernel 4.10
+		 */
+#ifdef NAPI_GRO_FLUSH_2_PARAMS
+		napi_gro_flush(rq->cq.napi, false);
+#else
+		napi_gro_flush(rq->cq.napi);
+#endif
+#endif
 	}
 
 	if (unlikely(mlx5e_rx_hw_stamp(rq->tstamp)))
@@ -954,18 +1021,33 @@ static inline void mlx5e_build_rx_skb(struct mlx5_cqe64 *cqe,
 
 	skb_record_rx_queue(skb, rq->ix);
 
+#ifdef HAVE_NETIF_F_RXHASH
 	if (likely(netdev->features & NETIF_F_RXHASH))
 		mlx5e_skb_set_hash(cqe, skb);
+#endif
 
 	if (cqe_has_vlan(cqe)) {
+#ifdef HAVE_3_PARAMS_FOR_VLAN_HWACCEL_PUT_TAG
 		__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),
 				       be16_to_cpu(cqe->vlan_info));
+#else
+		__vlan_hwaccel_put_tag(skb, be16_to_cpu(cqe->vlan_info));
+#endif
 		stats->removed_vlan_packets++;
 	}
 
 	skb->mark = be32_to_cpu(cqe->sop_drop_qpn) & MLX5E_TC_FLOW_ID_MASK;
 
+#ifndef CONFIG_COMPAT_LRO_ENABLED_IPOIB
 	mlx5e_handle_csum(netdev, cqe, rq, skb, !!lro_num_seg);
+#else
+	l4_hdr_type = get_cqe_l4_hdr_type(cqe);
+	mlx5e_handle_csum(netdev, cqe, rq, skb,
+			  !!lro_num_seg ||
+			  (IS_SW_LRO(&priv->channels.params) &&
+			  (l4_hdr_type != CQE_L4_HDR_TYPE_NONE) &&
+			  (l4_hdr_type != CQE_L4_HDR_TYPE_UDP)));
+#endif
 	/* checking CE bit in cqe - MSB in ml_path field */
 	if (unlikely(cqe->ml_path & MLX5E_CE_BIT_MASK))
 		mlx5e_enable_ecn(rq, skb);
@@ -994,6 +1076,42 @@ static inline void mlx5e_complete_rx_cqe(struct mlx5e_rq *rq,
 	}
 }
 
+#ifndef HAVE_BUILD_SKB
+static inline struct sk_buff *mlx5e_compat_build_skb(struct mlx5e_rq *rq,
+						struct page *page,
+						u32 cqe_bcnt,
+						unsigned int offset)
+{
+	u16 headlen = min_t(u32, MLX5E_RX_MAX_HEAD, cqe_bcnt);
+	u32 frag_size = cqe_bcnt - headlen;
+	struct sk_buff *skb;
+	void *head_ptr = page_address(page) + offset + rq->buff.headroom;
+
+	skb = netdev_alloc_skb(rq->netdev, headlen + rq->buff.headroom);
+	if (unlikely(!skb))
+		return NULL;
+
+	if (frag_size) {
+		u32 frag_offset = offset + rq->buff.headroom + headlen;
+		unsigned int truesize =	SKB_TRUESIZE(frag_size);
+
+		skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,
+				page, frag_offset,
+				frag_size, truesize);
+	}
+
+	/* copy header */
+	skb_reserve(skb, rq->buff.headroom);
+	skb_copy_to_linear_data(skb, head_ptr, headlen);
+
+	/* skb linear part was allocated with headlen and aligned to long */
+	skb->tail += headlen;
+	skb->len  += headlen;
+	return skb;
+}
+#endif
+
+#ifdef HAVE_BUILD_SKB
 static inline
 struct sk_buff *mlx5e_build_linear_skb(struct mlx5e_rq *rq, void *va,
 				       u32 frag_size, u16 headroom,
@@ -1011,6 +1129,7 @@ struct sk_buff *mlx5e_build_linear_skb(struct mlx5e_rq *rq, void *va,
 
 	return skb;
 }
+#endif
 
 struct sk_buff *
 mlx5e_skb_from_cqe_linear(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe,
@@ -1020,7 +1139,9 @@ mlx5e_skb_from_cqe_linear(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe,
 	u16 rx_headroom = rq->buff.headroom;
 	struct sk_buff *skb;
 	void *va, *data;
+#ifdef HAVE_XDP_BUFF
 	bool consumed;
+#endif
 	u32 frag_size;
 
 	va             = page_address(di->page) + wi->offset;
@@ -1037,17 +1158,26 @@ mlx5e_skb_from_cqe_linear(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe,
 		return NULL;
 	}
 
+#ifdef HAVE_XDP_BUFF
 	rcu_read_lock();
 	consumed = mlx5e_xdp_handle(rq, di, va, &rx_headroom, &cqe_bcnt);
 	rcu_read_unlock();
 	if (consumed)
 		return NULL; /* page/packet was consumed by XDP */
+#endif
 
+#ifdef HAVE_BUILD_SKB
 	skb = mlx5e_build_linear_skb(rq, va, frag_size, rx_headroom, cqe_bcnt);
+#else
+	skb = mlx5e_compat_build_skb(rq, di->page, cqe_bcnt, wi->offset);
+#endif
 	if (unlikely(!skb))
 		return NULL;
 
 	/* queue up for recycling/reuse */
+#ifndef HAVE_BUILD_SKB
+	if (skb_shinfo(skb)->nr_frags)
+#endif
 	page_ref_inc(di->page);
 
 	return skb;
@@ -1072,7 +1202,11 @@ mlx5e_skb_from_cqe_nonlinear(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe,
 	/* XDP is not supported in this configuration, as incoming packets
 	 * might spread among multiple pages.
 	 */
+#ifdef HAVE_NAPI_ALLOC_SKB
 	skb = napi_alloc_skb(rq->cq.napi,
+#else
+	skb = netdev_alloc_skb_ip_align(rq->netdev,
+#endif
 			     ALIGN(MLX5E_RX_MAX_HEAD, sizeof(long)));
 	if (unlikely(!skb)) {
 		rq->stats->buff_alloc_err++;
@@ -1105,6 +1239,9 @@ mlx5e_skb_from_cqe_nonlinear(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe,
 
 void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
 {
+#if defined(HAVE_VLAN_GRO_RECEIVE) || defined(HAVE_VLAN_HWACCEL_RX) || defined(CONFIG_COMPAT_LRO_ENABLED_IPOIB)
+	struct mlx5e_priv *priv = netdev_priv(rq->netdev);
+#endif
 	struct mlx5_wq_cyc *wq = &rq->wqe.wq;
 	struct mlx5e_wqe_frag_info *wi;
 	struct sk_buff *skb;
@@ -1128,6 +1265,31 @@ void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
 	}
 
 	mlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	if (IS_SW_LRO(&priv->channels.params))
+#if defined(HAVE_VLAN_GRO_RECEIVE) || defined(HAVE_VLAN_HWACCEL_RX)
+		if (priv->channels.params.vlan_grp && cqe_has_vlan(cqe))
+			lro_vlan_hwaccel_receive_skb(&rq->sw_lro->lro_mgr,
+						     skb, priv->channels.params.vlan_grp,
+						     be16_to_cpu(cqe->vlan_info),
+						     NULL);
+		else
+#endif
+		lro_receive_skb(&rq->sw_lro->lro_mgr, skb, NULL);
+	else
+#endif
+#if defined(HAVE_VLAN_GRO_RECEIVE) || defined(HAVE_VLAN_HWACCEL_RX)
+                if (priv->channels.params.vlan_grp && cqe_has_vlan(cqe))
+#ifdef HAVE_VLAN_GRO_RECEIVE
+                        vlan_gro_receive(rq->cq.napi, priv->channels.params.vlan_grp,
+                                         be16_to_cpu(cqe->vlan_info),
+                                         skb);
+#else
+                        vlan_hwaccel_receive_skb(skb, priv->channels.params.vlan_grp,
+                                        be16_to_cpu(cqe->vlan_info));
+#endif
+		else
+#endif
 	napi_gro_receive(rq->cq.napi, skb);
 
 free_wqe:
@@ -1139,10 +1301,14 @@ wq_cyc_pop:
 #ifdef CONFIG_MLX5_ESWITCH
 void mlx5e_handle_rx_cqe_rep(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
 {
+#if defined(HAVE_SKB_VLAN_POP) || defined(HAVE_VLAN_GRO_RECEIVE) || defined(HAVE_VLAN_HWACCEL_RX)
 	struct net_device *netdev = rq->netdev;
 	struct mlx5e_priv *priv = netdev_priv(netdev);
+#ifdef HAVE_SKB_VLAN_POP
 	struct mlx5e_rep_priv *rpriv  = priv->ppriv;
 	struct mlx5_eswitch_rep *rep = rpriv->rep;
+#endif
+#endif
 	struct mlx5_wq_cyc *wq = &rq->wqe.wq;
 	struct mlx5e_wqe_frag_info *wi;
 	struct sk_buff *skb;
@@ -1167,9 +1333,23 @@ void mlx5e_handle_rx_cqe_rep(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
 
 	mlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
 
+#ifdef HAVE_SKB_VLAN_POP
 	if (rep->vlan && skb_vlan_tag_present(skb))
 		skb_vlan_pop(skb);
+#endif
 
+#if defined(HAVE_VLAN_GRO_RECEIVE) || defined(HAVE_VLAN_HWACCEL_RX)
+	if (priv->channels.params.vlan_grp && cqe_has_vlan(cqe))
+#ifdef HAVE_VLAN_GRO_RECEIVE
+		vlan_gro_receive(rq->cq.napi, priv->channels.params.vlan_grp,
+				 be16_to_cpu(cqe->vlan_info),
+				 skb);
+#else
+	vlan_hwaccel_receive_skb(skb, priv->channels.params.vlan_grp,
+				 be16_to_cpu(cqe->vlan_info));
+#endif
+	else
+#endif
 	napi_gro_receive(rq->cq.napi, skb);
 
 free_wqe:
@@ -1190,7 +1370,11 @@ mlx5e_skb_from_cqe_mpwrq_nonlinear(struct mlx5e_rq *rq, struct mlx5e_mpw_info *w
 	struct mlx5e_dma_info *head_di = di;
 	struct sk_buff *skb;
 
+#ifdef HAVE_NAPI_ALLOC_SKB
 	skb = napi_alloc_skb(rq->cq.napi,
+#else
+	skb = netdev_alloc_skb_ip_align(rq->netdev,
+#endif
 			     ALIGN(MLX5E_RX_MAX_HEAD, sizeof(long)));
 	if (unlikely(!skb)) {
 		rq->stats->buff_alloc_err++;
@@ -1236,7 +1420,9 @@ mlx5e_skb_from_cqe_mpwrq_linear(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi,
 	struct sk_buff *skb;
 	void *va, *data;
 	u32 frag_size;
+#ifdef HAVE_XDP_BUFF
 	bool consumed;
+#endif
 
 	/* Check packet size. Note LRO doesn't use linear SKB. */
 	if (unlikely(cqe_bcnt > rq->hw_mtu)) {
@@ -1253,6 +1439,7 @@ mlx5e_skb_from_cqe_mpwrq_linear(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi,
 	prefetchw(va); /* xdp_frame data area */
 	prefetch(data);
 
+#ifdef HAVE_XDP_BUFF
 	rcu_read_lock();
 	consumed = mlx5e_xdp_handle(rq, di, va, &rx_headroom, &cqe_bcnt32);
 	rcu_read_unlock();
@@ -1261,12 +1448,20 @@ mlx5e_skb_from_cqe_mpwrq_linear(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi,
 			__set_bit(page_idx, wi->xdp_xmit_bitmap); /* non-atomic */
 		return NULL; /* page/packet was consumed by XDP */
 	}
+#endif
 
+#ifdef HAVE_BUILD_SKB
 	skb = mlx5e_build_linear_skb(rq, va, frag_size, rx_headroom, cqe_bcnt32);
+#else
+	skb = mlx5e_compat_build_skb(rq, di->page, cqe_bcnt32, head_offset);
+#endif
 	if (unlikely(!skb))
 		return NULL;
 
 	/* queue up for recycling/reuse */
+#ifndef HAVE_BUILD_SKB
+	if (skb_shinfo(skb)->nr_frags)
+#endif
 	page_ref_inc(di->page);
 
 	return skb;
@@ -1274,6 +1469,9 @@ mlx5e_skb_from_cqe_mpwrq_linear(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi,
 
 void mlx5e_handle_rx_cqe_mpwrq(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
 {
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct mlx5e_priv *priv = netdev_priv(rq->netdev);
+#endif
 	u16 cstrides       = mpwrq_get_cqe_consumed_strides(cqe);
 	u16 wqe_id         = be16_to_cpu(cqe->wqe_id);
 	struct mlx5e_mpw_info *wi = &rq->mpwqe.info[wqe_id];
@@ -1309,6 +1507,31 @@ void mlx5e_handle_rx_cqe_mpwrq(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
 		goto mpwrq_cqe_out;
 
 	mlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	if (IS_SW_LRO(&priv->channels.params))
+#if defined(HAVE_VLAN_GRO_RECEIVE) || defined(HAVE_VLAN_HWACCEL_RX)
+		if (priv->channels.params.vlan_grp && cqe_has_vlan(cqe))
+			lro_vlan_hwaccel_receive_skb(&rq->sw_lro->lro_mgr,
+						     skb, priv->channels.params.vlan_grp,
+						     be16_to_cpu(cqe->vlan_info),
+						     NULL);
+		else
+#endif
+		lro_receive_skb(&rq->sw_lro->lro_mgr, skb, NULL);
+	else
+#endif
+#if defined(HAVE_VLAN_GRO_RECEIVE) || defined(HAVE_VLAN_HWACCEL_RX)
+                if (priv->channels.params.vlan_grp && cqe_has_vlan(cqe))
+#ifdef HAVE_VLAN_GRO_RECEIVE
+                        vlan_gro_receive(rq->cq.napi, priv->channels.params.vlan_grp,
+                                         be16_to_cpu(cqe->vlan_info),
+                                         skb);
+#else
+                        vlan_hwaccel_receive_skb(skb, priv->channels.params.vlan_grp,
+                                        be16_to_cpu(cqe->vlan_info));
+#endif
+		else
+#endif
 	napi_gro_receive(rq->cq.napi, skb);
 
 mpwrq_cqe_out:
@@ -1324,9 +1547,20 @@ mpwrq_cqe_out:
 int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget)
 {
 	struct mlx5e_rq *rq = container_of(cq, struct mlx5e_rq, cq);
+#ifdef HAVE_XDP_BUFF
 	struct mlx5e_xdpsq *xdpsq = &rq->xdpsq;
+#endif
 	struct mlx5_cqe64 *cqe;
 	int work_done = 0;
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct mlx5e_priv *priv;
+#ifdef CONFIG_MLX5_CORE_IPOIB
+	if (MLX5_CAP_GEN(cq->mdev, port_type) != MLX5_CAP_PORT_TYPE_ETH)
+		priv = mlx5i_epriv(rq->netdev);
+	else
+#endif
+		priv = netdev_priv(rq->netdev);
+#endif
 
 	if (unlikely(!test_bit(MLX5E_RQ_STATE_ENABLED, &rq->state)))
 		return 0;
@@ -1340,7 +1574,9 @@ int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget)
 			goto out;
 		return 0;
 	}
-
+#ifdef HAVE_XDP_BUFF
+	xdpsq = &rq->xdpsq;
+#endif
 	do {
 		if (mlx5_get_cqe_format(cqe) == MLX5_COMPRESSED) {
 			work_done +=
@@ -1355,21 +1591,30 @@ int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget)
 	} while ((++work_done < budget) && (cqe = mlx5_cqwq_get_cqe(&cq->wq)));
 
 out:
+#ifdef HAVE_XDP_BUFF
 	if (xdpsq->doorbell) {
 		mlx5e_xmit_xdp_doorbell(xdpsq);
 		xdpsq->doorbell = false;
 	}
 
+#ifdef HAVE_XDP_REDIRECT
 	if (xdpsq->redirect_flush) {
 		xdp_do_flush_map();
 		xdpsq->redirect_flush = false;
 	}
+#endif
+#endif
 
 	mlx5_cqwq_update_db_record(&cq->wq);
 
 	/* ensure cq space is freed before enabling more cqes */
 	wmb();
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	if (IS_SW_LRO(&priv->channels.params))
+		lro_flush_all(&rq->sw_lro->lro_mgr);
+#endif
+
 	return work_done;
 }
 
@@ -1391,6 +1636,9 @@ static inline void mlx5i_complete_rx_cqe(struct mlx5e_rq *rq,
 	u32 qpn;
 	u8 *dgid;
 	u8 g;
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+       struct mlx5e_priv *parent_priv = mlx5i_epriv(rq->netdev);
+#endif
 
 	qpn = be32_to_cpu(cqe->sop_drop_qpn) & 0xffffff;
 	netdev = mlx5i_pkey_get_netdev(rq->netdev, qpn);
@@ -1426,6 +1674,11 @@ static inline void mlx5i_complete_rx_cqe(struct mlx5e_rq *rq,
 
 	skb->protocol = *((__be16 *)(skb->data));
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	if (parent_priv->netdev->features & NETIF_F_LRO) {
+		skb->ip_summed = CHECKSUM_UNNECESSARY;
+	} else
+#endif
 	if (netdev->features & NETIF_F_RXCSUM) {
 		skb->ip_summed = CHECKSUM_COMPLETE;
 		skb->csum = csum_unfold((__force __sum16)cqe->check_sum);
@@ -1441,8 +1694,10 @@ static inline void mlx5i_complete_rx_cqe(struct mlx5e_rq *rq,
 
 	skb_record_rx_queue(skb, rq->ix);
 
+#ifdef HAVE_NETIF_F_RXHASH
 	if (likely(netdev->features & NETIF_F_RXHASH))
 		mlx5e_skb_set_hash(cqe, skb);
+#endif
 
 	/* 20 bytes of ipoib header and 4 for encap existing */
 	pseudo_header = skb_push(skb, MLX5_IPOIB_PSEUDO_LEN);
@@ -1458,6 +1713,9 @@ static inline void mlx5i_complete_rx_cqe(struct mlx5e_rq *rq,
 
 void mlx5i_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
 {
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct mlx5e_priv *priv = mlx5i_epriv(rq->netdev);
+#endif
 	struct mlx5_wq_cyc *wq = &rq->wqe.wq;
 	struct mlx5e_wqe_frag_info *wi;
 	struct sk_buff *skb;
@@ -1477,6 +1735,11 @@ void mlx5i_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
 		dev_kfree_skb_any(skb);
 		goto wq_free_wqe;
 	}
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	if (priv->netdev->features & NETIF_F_LRO)
+		lro_receive_skb(&rq->sw_lro->lro_mgr, skb, NULL);
+	else
+#endif
 	napi_gro_receive(rq->cq.napi, skb);
 
 wq_free_wqe:
@@ -1490,6 +1753,9 @@ wq_free_wqe:
 
 void mlx5e_ipsec_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
 {
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct mlx5e_priv *priv = mlx5i_epriv(rq->netdev);
+#endif
 	struct mlx5_wq_cyc *wq = &rq->wqe.wq;
 	struct mlx5e_wqe_frag_info *wi;
 	struct sk_buff *skb;
